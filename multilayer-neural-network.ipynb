{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSsAYIr0TveR"
   },
   "source": [
    "# Multi-Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  We're implementing a multi layer neural network for animal classification. Our network will consist of one input layer, n hidden layers and one output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "yBuAC4ZNW2Fz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image, ImageOps\n",
    "np.random.seed(35819)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " First we generate our input layer. This function takes the given image and converts it into an array of its pixel values. Then normalizes the values which are 0-255 to between 0 and 1."
   ]
  },
  
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_layer(image_path: str, input_layer_size: int=2500):\n",
    "    dim = int(np.sqrt(input_layer_size))\n",
    "    image = Image.open(image_path).resize((dim, dim))\n",
    "    image = ImageOps.grayscale(image)\n",
    "    imdata = np.asarray(image)\n",
    "    imdata -= int(np.mean(imdata))\n",
    "    imdata = (imdata - np.min(imdata)) / (np.max(imdata) - np.min(imdata)) if (np.max(imdata) - np.min(imdata)) != 0 else imdata-imdata # squishing integer vals between 0-255 into 0-1\n",
    "    return imdata.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our true label should be in the format of one hot encoding in order to compare to the predicted value easily therefore we're converting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"dog\", \"horse\", \"elephant\", \"butterfly\", \"chicken\", \"cat\", \"cow\", \"sheep\", \"spider\", \"squirrel\"]\n",
    "def one_hot(label):\n",
    "    onehot = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    for i in range(10):\n",
    "        if label == classes[i]: onehot[i] = 1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We're reading the images and creating the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "translate = {\"cane\": \"dog\", \"cavallo\": \"horse\", \"elefante\": \"elephant\", \"farfalla\": \"butterfly\", \"gallina\": \"chicken\", \"gatto\": \"cat\", \"mucca\": \"cow\", \"pecora\": \"sheep\", \"ragno\": \"spider\", \"scoiattolo\": \"squirrel\", \"dog\": \"cane\", \"horse\": \"cavallo\", \"elephant\" : \"elefante\", \"butterfly\": \"farfalla\", \"chicken\": \"gallina\", \"cat\": \"gatto\", \"cow\": \"mucca\", \"spider\": \"ragno\", \"squirrel\": \"scoiattolo\", \"sheep\": \"pecora\"}\n",
    "for filename in os.listdir(\"archive/raw-img/\"):\n",
    "    if filename == \".DS_Store\": continue\n",
    "    filepath = \"archive/raw-img/\"+filename+\"/\"\n",
    "    for image in os.listdir(filepath):\n",
    "        inp = generate_input_layer(filepath+image)\n",
    "        images.append(inp)\n",
    "        labels.append(one_hot(translate[filename]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "images = np.concatenate((images, labels), axis=1)\n",
    "np.random.shuffle(images)\n",
    "trainingSet, testSet = train_test_split(images, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're initializing the weights randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(inputLayerSize, hiddenLayerNo, hiddenLayerSize, outputLayerSize):\n",
    "    layer = list()\n",
    "    for i in range(hiddenLayerNo+1):\n",
    "        if hiddenLayerNo == 0: \n",
    "            weight = np.random.randn(inputLayerSize, outputLayerSize) * np.sqrt(2.0 / (inputLayerSize + outputLayerSize))\n",
    "        elif i == 0: \n",
    "            weight = np.random.randn(inputLayerSize, hiddenLayerSize) * np.sqrt(2.0 / (inputLayerSize + hiddenLayerSize))\n",
    "        elif i == hiddenLayerNo: \n",
    "            weight = np.random.randn(hiddenLayerSize, outputLayerSize)  * np.sqrt(2.0 / (hiddenLayerSize + outputLayerSize))\n",
    "        else: \n",
    "            weight = np.random.randn(hiddenLayerSize, hiddenLayerSize)  * np.sqrt(2.0 / (hiddenLayerSize + hiddenLayerSize))\n",
    "        layer.append({\"W\": weight})\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our activations functions to help the network learn complex patterns in the data, softmax function that used for the output layer of neural network models that predict a multinomial probability distribution, loss function to calculate the error and their derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "\treturn 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(S, grad): #S is output of sigmoid function\n",
    "    return S * (1 - S) * grad\n",
    "\n",
    "def reLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def reLU_deriv(S, grad): #S is output of reLU function\n",
    "    return (S > 0) * 1 * grad\n",
    "\n",
    "def tanH(x):\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "def tanH_deriv(S, grad): #S is output of tanH function\n",
    "    return (1 - np.square(S)) * grad\n",
    "        \n",
    "def softmax(vector):\n",
    "    e = np.exp(vector)\n",
    "    return e / e.sum(axis=1, keepdims=True)\n",
    "\n",
    "def softmax_deriv(SM, grad): #SM is output of softmax function\n",
    "    return (np.sum(grad * -SM, axis=1, keepdims=True)+grad) * SM\n",
    "\n",
    "def linear_derivX(grad, index, layer): #derivative of loss w.r.t X calculated for the chain rule\n",
    "    dX = np.dot(grad, np.transpose(layer[index][\"W\"]))\n",
    "    return dX\n",
    "\n",
    "def linear_deriv(grad, index, layer):\n",
    "    layer[index][\"dW\"] = np.dot(np.transpose(layer[index-1][\"X\"]), grad)\n",
    "    layer[index][\"db\"] = grad.sum(axis=0)\n",
    "    return layer\n",
    "\n",
    "def get_loss(batch, layer):\n",
    "    return np.sum(np.dot(-np.log(layer[-1][\"SM\"]), np.transpose(batch[:,2500:])))\n",
    "\n",
    "def nll_deriv(labels, predictions):\n",
    "    return -labels/predictions\n",
    "\n",
    "def bias(size, layerNo, layer):\n",
    "    layer[layerNo][\"b\"] = np.full((size,), 0)\n",
    "    return layer[layerNo][\"b\"]\n",
    "\n",
    "def set_biases(layer, hiddenLayerSize):  #initializing bias arrays with 0s for each layer\n",
    "    for l in range(len(layer)):\n",
    "        if l == len(layer)-1:\n",
    "            bias(10, l, layer)\n",
    "        else:\n",
    "            bias(hiddenLayerSize, l, layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here we calculate the X values for each layer. Then pass them to activation functions and finally in the output layer, to softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(batch, sig, relu, layer):\n",
    "    for i in range(1, len(layer)):\n",
    "        if i == 1:\n",
    "            X = np.add(np.dot(batch[:,:2500], layer[i][\"W\"]), layer[i][\"b\"]) #Oi=WijXj+bi\n",
    "        elif i == len(layer)-1:\n",
    "            X = np.add(np.dot(layer[i-1][\"S\"], layer[i][\"W\"],), layer[i][\"b\"])\n",
    "        else:\n",
    "            X = np.add(np.dot(layer[i-1][\"S\"], layer[i][\"W\"]), layer[i][\"b\"]) \n",
    "        layer[i][\"X\"] = X\n",
    "        if sig: \n",
    "            layer[i][\"S\"] = sigmoid(X)\n",
    "        elif relu:\n",
    "            layer[i][\"S\"] = reLU(X)\n",
    "        else:\n",
    "            layer[i][\"S\"] = tanH(X)\n",
    "    layer[i][\"SM\"] = softmax(layer[i][\"S\"])\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the derivative of loss with respect to the weights is calculated by using the chain rule. The chain rule enables the network to identify how much each weight contributes to the error and how much each weight needs to be adjusted. Also derivative of loss w.r.t biases is calculated to update them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(hiddenLayerCount, batch, sigmoid, relu, layer):\n",
    "    func = sigmoid_deriv if sigmoid else reLU_deriv\n",
    "    func = reLU_deriv if relu else tanH_deriv\n",
    "    grad = nll_deriv(batch[:,2500:], layer[-1][\"SM\"])\n",
    "    grad = softmax_deriv(layer[-1][\"SM\"], grad)\n",
    "    for i in range(hiddenLayerCount+1):\n",
    "        grad = func(layer[hiddenLayerCount-i+1][\"S\"], grad)\n",
    "        layer = linear_deriv(grad, hiddenLayerCount-i+1, layer)\n",
    "        grad = linear_derivX(grad, hiddenLayerCount-i+1, layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We multiply the derivatives of the loss w.r.t weight and biases by learning rate and the weights and biases are updated accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(hiddenLayerCount, learningRate, layer):\n",
    "    for i in range(-1, hiddenLayerCount):\n",
    "        layer[hiddenLayerCount-i][\"W\"] =  np.subtract(layer[hiddenLayerCount-i][\"W\"], learningRate * layer[hiddenLayerCount-i][\"dW\"])\n",
    "        layer[hiddenLayerCount-i][\"b\"] = np.subtract(layer[hiddenLayerCount-i][\"b\"], learningRate * layer[hiddenLayerCount-i][\"db\"])\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we multiplied the learning rate by some decay rate for it to converge better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batchSize, epochNo, hiddenLayerCount, learningRate, inputLayerSize, sig, relu):\n",
    "    hiddenLayerSize = int(2*(inputLayerSize+10)/3)\n",
    "    layer = [{}]+initialize_weights(inputLayerSize, hiddenLayerCount, hiddenLayerSize, 10)\n",
    "    imageBatches = np.split(trainingSet, np.arange(batchSize, len(trainingSet), batchSize))\n",
    "    imageBatches = np.delete(imageBatches, len(imageBatches)-1, 0)\n",
    "    set_biases(layer, hiddenLayerSize)\n",
    "    initial = learningRate\n",
    "    for batch in imageBatches:\n",
    "        learningRate = initial\n",
    "        layer = get_activations(batch, sig, relu, layer)\n",
    "        layer[0][\"X\"] = batch[:,:inputLayerSize]\n",
    "        for epoch in range(epochNo):\n",
    "            learningRate =(1/(1+10*epoch))*learningRate\n",
    "            gradient(hiddenLayerCount, batch, sig, relu, layer)\n",
    "            layer = update(hiddenLayerCount, learningRate, layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are converting the output into one hot encoding by taking the maximum value in the predictions and classify the test as of that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(output):\n",
    "    ind = np.argmax(output, axis=1)\n",
    "    onehot = np.zeros((len(ind), 10))\n",
    "    for i in range(len(ind)):\n",
    "        onehot[i,ind[i]] = 1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(batchSize, inputLayerSize, sig, relu, epochNo, hiddenLayerCount, learningRate):\n",
    "    layer = train(batchSize, epochNo, hiddenLayerCount, learningRate, 2500, sig, relu)\n",
    "    testBatches = np.split(trainingSet, np.arange(batchSize, len(trainingSet), batchSize))\n",
    "    testBatches = np.delete(testBatches, len(testBatches)-1, 0)\n",
    "    totalAccuracy = 0\n",
    "    for testBatch in testBatches:\n",
    "        layer = get_activations(testBatch, sig, relu, layer)\n",
    "        predictions = to_onehot(layer[-1][\"SM\"])\n",
    "        accuracy = predictions * testBatch[:, inputLayerSize:]\n",
    "        accuracy = np.sum(accuracy)/accuracy.shape[0]*100\n",
    "        totalAccuracy += accuracy\n",
    "    return totalAccuracy/len(testBatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "learningRates = [0.005, 0.01, 0.02]\n",
    "batchSizes = [16, 64, 128]\n",
    "hiddenUnitNos = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Even though we used a decay parameter, the values given were too large and they didn't converge. Therefore all of them has the same accuracy and don't give us much information. For the next trainings we used a smaller learning rate which is 0.0001 and got better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d6/p_p19vt54p1gb7kxxflt5tk00000gn/T/ipykernel_26477/3370038240.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
      "/var/folders/d6/p_p19vt54p1gb7kxxflt5tk00000gn/T/ipykernel_26477/3370038240.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════════════╤════════════╕\n",
      "│   Learning Rate │   Accuracy │\n",
      "╞═════════════════╪════════════╡\n",
      "│           0.005 │    18.7069 │\n",
      "├─────────────────┼────────────┤\n",
      "│           0.01  │    18.7069 │\n",
      "├─────────────────┼────────────┤\n",
      "│           0.02  │    18.7069 │\n",
      "╘═════════════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "learningRateAccuracies = [[\"Learning Rate\", \"Accuracy\"], [learningRates[0], test(128, 2500, 0, 0, 10, 2, learningRates[0])], [learningRates[1], test(128, 2500, 0, 0, 10, 2, learningRates[1])],[learningRates[2], test(128, 2500, 0, 0, 10, 2, learningRates[2])]]\n",
    "print(tabulate(learningRateAccuracies, headers='firstrow', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As batch size increases, the number of iterations decrease which means error will be minimized less times. Therefore selecting a smaller batch size gives us better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════════╤════════════╕\n",
      "│   Batch Size │   Accuracy │\n",
      "╞══════════════╪════════════╡\n",
      "│           16 │    26.5721 │\n",
      "├──────────────┼────────────┤\n",
      "│           64 │    26.5243 │\n",
      "├──────────────┼────────────┤\n",
      "│          128 │    26.4571 │\n",
      "╘══════════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "batchSizeAccuracies = [[\"Batch Size\", \"Accuracy\"], [batchSizes[0], test(batchSizes[0], 2500, 0, 1, 10, 2, 0.0001)], [batchSizes[1], test(batchSizes[1], 2500, 0, 1, 10, 2, 0.0001)],[batchSizes[2], test(batchSizes[2], 2500, 0, 1, 10, 2, 0.0001)]]\n",
    "print(tabulate(batchSizeAccuracies, headers='firstrow', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If the data is linearly separable we don't need any hidden layers at all. Other than that, one hidden layer is sufficient for the large majority of problems. Problems that require two hidden layers are rarely encountered. However, neural networks with two hidden layers can represent functions with any kind of shape. In our case, 2 layers worked better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════════════╤════════════╕\n",
      "│   Hidden Unit No │   Accuracy │\n",
      "╞══════════════════╪════════════╡\n",
      "│                0 │    21.8558 │\n",
      "├──────────────────┼────────────┤\n",
      "│                1 │    21.3765 │\n",
      "├──────────────────┼────────────┤\n",
      "│                2 │    26.5194 │\n",
      "╘══════════════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "hiddenUnitAccuracies = [[\"Hidden Unit No\", \"Accuracy\"], [hiddenUnitNos[0], test(128, 2500, 0, 1, 10, hiddenUnitNos[0], 0.0001)], [hiddenUnitNos[1], test(128, 2500, 0, 1, 10, hiddenUnitNos[1], 0.0001)],[hiddenUnitNos[2], test(128, 2500, 0, 1, 10, hiddenUnitNos[2], 0.0001)]]\n",
    "print(tabulate(hiddenUnitAccuracies, headers='firstrow', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. For sigmoid and tanH functions, this vanishing gradient problem makes the learning process very slow and make them converge to their optimum. However for reLU function, maximum threshold is infinity and there is no issue of vanishing gradient problem, it shows better convergence therefore the accuracy is maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════════════╤════════════╕\n",
      "│ Activation Function   │   Accuracy │\n",
      "╞═══════════════════════╪════════════╡\n",
      "│ Sigmoid               │    19.9195 │\n",
      "├───────────────────────┼────────────┤\n",
      "│ ReLU                  │    25.2109 │\n",
      "├───────────────────────┼────────────┤\n",
      "│ tanH                  │    18.2947 │\n",
      "╘═══════════════════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "ActivationFunctionAccuracies = [[\"Activation Function\", \"Accuracy\"], [\"Sigmoid\", test(128, 2500, 1, 0, 10, 2, 0.0001)], [\"ReLU\", test(128, 2500, 0, 1, 10, 2, 0.0001)],[\"tanH\", test(128, 2500, 0, 0, 10, 2, 0.0001)]]\n",
    "print(tabulate(ActivationFunctionAccuracies, headers='firstrow', tablefmt='fancy_grid'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "report.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "e8ee52e26650204a08c885461d142fd8f9417f3debf9f56bc8c004b34840a697"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
